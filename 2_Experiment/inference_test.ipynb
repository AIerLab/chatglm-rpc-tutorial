{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/project-1/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-19 08:59:44,941] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/project-1/baoziyuan/ChatGLM-RPC-Tutorial/')\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from asyncio import sleep\n",
    "from queue import Queue\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "from chatglm_6b_split_server import ChatGLMTokenizer, ChatGLMConfig, ChatGLMForConditionalGeneration\n",
    "import os\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "from importlib import reload\n",
    "\n",
    "\n",
    "tokenizer_config_path = os.path.join(\"..\",\"..\",\"..\",\"checkpoints\",\"chatglm-6b\", 'tokenizer_config.json')\n",
    "model_config_path = os.path.join(\"..\",\"..\",\"..\",\"checkpoints\",\"chatglm-6b\", 'config.json')\n",
    "token_text_path = os.path.join(\"..\",\"..\",\"..\",\"checkpoints\",\"chatglm-6b\", \"ice_text.model\")\n",
    "model_dir = os.path.join(\"..\",\"..\",\"..\",\"checkpoints\",\"chatglm-6b\")\n",
    "checkpoint_path = os.path.join(\"output\",\"adgen-chatglm-6b-pt-128-2e-2\",\"checkpoint-3000\")\n",
    "model_state_dict_file_num = 8\n",
    "\n",
    "def load_model(model_dir):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "    config = AutoConfig.from_pretrained(model_dir, trust_remote_code=True, pre_seq_len=128)\n",
    "    model = AutoModel.from_pretrained(model_dir, config=config, trust_remote_code=True).half().cuda()\n",
    "    model = model.eval()\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:22<00:00,  2.86s/it]\n",
      "Some weights of ChatGLMForConditionalGeneration were not initialized from the model checkpoint at ../../../checkpoints/chatglm-6b and are newly initialized: ['transformer.prefix_encoder.embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGLMForConditionalGeneration(\n",
      "  (transformer): ChatGLMModel(\n",
      "    (word_embeddings): Embedding(130528, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x GLMBlock(\n",
      "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attention): SelfAttention(\n",
      "          (rotary_emb): RotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GLU(\n",
      "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "    (prefix_encoder): PrefixEncoder(\n",
      "      (embedding): Embedding(128, 229376)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=130528, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(model_dir)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utility\n",
    "#reload(utility)\n",
    "model.eval()\n",
    "hook_function = partial(utility.save_input_output, model)\n",
    "handle = model.transformer.layers[6].register_forward_hook(hook_function)\n",
    "#for name, param in model.named_parameters():\n",
    "#    param.requires_grad = True\n",
    "freeze_schedule = {5: [10, 11, 12], \n",
    "                10: [13, 14, 15]}\n",
    "epoch = 10\n",
    "utility.dynamic_freeze(model, freeze_schedule, epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'output/adgen-chatglm-6b-pt-128-2e-2/checkpoint-3000/pytorch_model.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prefix_state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(checkpoint_path, \u001b[39m\"\u001b[39;49m\u001b[39mpytorch_model.bin\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m      2\u001b[0m new_prefix_state_dict \u001b[39m=\u001b[39m {}\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m prefix_state_dict\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    272\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output/adgen-chatglm-6b-pt-128-2e-2/checkpoint-3000/pytorch_model.bin'"
     ]
    }
   ],
   "source": [
    "prefix_state_dict = torch.load(os.path.join(checkpoint_path, \"pytorch_model.bin\"))\n",
    "new_prefix_state_dict = {}\n",
    "for k, v in prefix_state_dict.items():\n",
    "    new_prefix_state_dict[k[len(\"transformer.prefix_encoder.\"):]] = v\n",
    "model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)\n",
    "\n",
    "#V100 机型上可以不进行量化\n",
    "#print(f\"Quantized to 4 bit\")\n",
    "#model = model.quantize(4)\n",
    "model = model.half().cuda()\n",
    "model.transformer.prefix_encoder.float()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response, history = model.chat(tokenizer, \"类型#上衣\\*材质#牛仔布\\*颜色#白色\\*风格#简约\\*图案#刺绣\\*衣样式#外套\\*衣款式#破洞\", history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utility' from '/home/project-1/baoziyuan/ChatGLM-RPC-Tutorial/2_Experiment/utility.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "宫保鸡丁是一道非常有名的川菜，以下是制作宫保鸡丁的步骤：\n",
       "\n",
       "材料：\n",
       "\n",
       "- 鸡胸肉 1块\n",
       "- 青椒 1个\n",
       "- 干辣椒 10个\n",
       "- 姜 适量\n",
       "- 蒜 适量\n",
       "- 料酒 适量\n",
       "- 生抽 适量\n",
       "- 盐 适量\n",
       "- 糖 适量\n",
       "- 鸡精 适量\n",
       "- 淀粉 适量\n",
       "- 食用油 适量\n",
       "\n",
       "步骤：\n",
       "\n",
       "1. 将鸡胸肉切成丁，加入料酒、淀粉腌制15分钟。\n",
       "2. 干辣椒剪成段，姜、蒜切末备用。\n",
       "3. 青椒去籽，切成小块备用。\n",
       "4. 热锅凉油，将鸡丁煸炒至变色，加入干辣椒、姜蒜末继续煸炒。\n",
       "5. 加入适量的生抽、盐、糖、鸡精，翻炒均匀。\n",
       "6. 最后加入青椒块，继续翻炒几分钟即可。\n",
       "\n",
       "这是一道非常经典的川菜，希望您能喜欢。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "prompt = \"如何制作宫保鸡丁\"\n",
    "for response, history in model.stream_chat(\n",
    "        tokenizer, prompt, history=[]):\n",
    "    clear_output(wait=True)\n",
    "    display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4096])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "input_tensor = model.transformer.layers[0].input_tensor\n",
    "output_tensor = model.transformer.layers[0].output_tensor\n",
    "print(output_tensor.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
