{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/project-1/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-16 09:51:00,319] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/project-1/baoziyuan/ChatGLM-RPC-Tutorial/')\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from asyncio import sleep\n",
    "from queue import Queue\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from chatglm_6b_split_server import ChatGLMTokenizer, ChatGLMConfig, ChatGLMForConditionalGeneration\n",
    "import os\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "tokenizer_config_path = os.path.join(\"..\",\"..\",\"..\",\"checkpoints\",\"chatglm-6b\", 'tokenizer_config.json')\n",
    "model_config_path = os.path.join(\"..\",\"..\",\"..\",\"checkpoints\",\"chatglm-6b\", 'config.json')\n",
    "token_text_path = os.path.join(\"..\",\"..\",\"..\",\"checkpoints\",\"chatglm-6b\", \"ice_text.model\")\n",
    "model_dir = os.path.join(\"..\",\"..\",\"..\",\"checkpoints\",\"chatglm-6b\")\n",
    "checkpoint_path = os.path.join(\"output\",\"adgen-chatglm-6b-pt-128-2e-2\",\"checkpoint-3000\")\n",
    "model_state_dict_file_num = 8\n",
    "\n",
    "def load_model(model_dir):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "    config = AutoConfig.from_pretrained(model_dir, trust_remote_code=True, pre_seq_len=128)\n",
    "    model = AutoModel.from_pretrained(model_dir, config=config, trust_remote_code=True).half().cuda()\n",
    "    model = model.eval()\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:18<00:00,  2.28s/it]\n",
      "Some weights of ChatGLMForConditionalGeneration were not initialized from the model checkpoint at ../../../checkpoints/chatglm-6b and are newly initialized: ['transformer.prefix_encoder.embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGLMForConditionalGeneration(\n",
      "  (transformer): ChatGLMModel(\n",
      "    (word_embeddings): Embedding(130528, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x GLMBlock(\n",
      "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attention): SelfAttention(\n",
      "          (rotary_emb): RotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GLU(\n",
      "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "    (prefix_encoder): PrefixEncoder(\n",
      "      (embedding): Embedding(128, 229376)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=130528, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(model_dir)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_state_dict = torch.load(os.path.join(checkpoint_path, \"pytorch_model.bin\"))\n",
    "new_prefix_state_dict = {}\n",
    "for k, v in prefix_state_dict.items():\n",
    "    new_prefix_state_dict[k[len(\"transformer.prefix_encoder.\"):]] = v\n",
    "model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)\n",
    "\n",
    "#V100 机型上可以不进行量化\n",
    "#print(f\"Quantized to 4 bit\")\n",
    "#model = model.quantize(4)\n",
    "model = model.half().cuda()\n",
    "model.transformer.prefix_encoder.float()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRAND这款牛仔外套，简约的牛仔外套版型，没有多余的装饰，上身尽显干练气质。白色刺绣点缀，精致美观，彰显女性个性。破洞设计，个性时髦。\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"类型#上衣\\*材质#牛仔布\\*颜色#白色\\*风格#简约\\*图案#刺绣\\*衣样式#外套\\*衣款式#破洞\", history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "经典的宫保鸡丁，是女生们衣橱中必备的美味，而这款宫保鸡丁采用了<UNK>的面料，柔软舒适，穿着更加舒适。而这款宫保鸡丁在口味上，也进行了适当的改良，采用了甜度和酸度适中的酸角<UNK>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"如何制作宫保鸡丁\"\n",
    "for response, history in model.stream_chat(\n",
    "        tokenizer, prompt, history=[]):\n",
    "    clear_output(wait=True)\n",
    "    display(Markdown(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
