{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/project-1/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-12 11:52:30,103] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from asyncio import sleep\n",
    "from queue import Queue\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from chatglm_6b_split_server import ChatGLMTokenizer, ChatGLMConfig, ChatGLMForConditionalGeneration\n",
    "import os\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "tokenizer_config_path = os.path.join(\"..\",\"..\",\"..\",\"checkpoints\",\"chatglm-6b\", 'tokenizer_config.json')\n",
    "model_config_path = os.path.join(\"..\",\"..\",\"..\",\"checkpoints\",\"chatglm-6b\", 'config.json')\n",
    "token_text_path = os.path.join(\"..\",\"..\",\"..\",\"checkpoints\",\"chatglm-6b\", \"ice_text.model\")\n",
    "model_dir = os.path.join(\"..\",\"..\",\"..\",\"checkpoints\",\"chatglm-6b\")\n",
    "model_state_dict_file_num = 8\n",
    "\n",
    "def load_model():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "    model = AutoModel.from_pretrained(model_dir, trust_remote_code=True).half().cuda()\n",
    "    model = model.eval()\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:03<00:03,  1.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:06<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGLMForConditionalGeneration(\n",
      "  (transformer): ChatGLMModel(\n",
      "    (word_embeddings): Embedding(130528, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x GLMBlock(\n",
      "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attention): SelfAttention(\n",
      "          (rotary_emb): RotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GLU(\n",
      "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=130528, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The dtype of attention mask (torch.int64) is not bool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‰Ω†Â•ΩüëãÔºÅÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã ChatGLM-6BÔºåÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†ÔºåÊ¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"‰Ω†Â•Ω\", history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Âà∂‰ΩúÂÆ´‰øùÈ∏°‰∏ÅÁöÑÂÖ∑‰ΩìÊñπÊ≥ïÂ¶Ç‰∏ãÔºö\n",
       "\n",
       "ÊùêÊñôÔºö\n",
       "- È∏°ËÉ∏ËÇâ 200 ÂÖã\n",
       "- Âπ≤Ëæ£Ê§í 10 ‰∏™\n",
       "- Ëë±ÂßúËíúÈÄÇÈáè\n",
       "- ÊñôÈÖí„ÄÅÁõê„ÄÅÁ≥ñ„ÄÅÁîüÊäΩ„ÄÅÈÜã„ÄÅÊ∑ÄÁ≤â„ÄÅÊ≤πÈÄÇÈáè\n",
       "\n",
       "Ê≠•È™§Ôºö\n",
       "\n",
       "1. È∏°ËÉ∏ËÇâÂàáÊàê‰∏ÅÔºåÁî®ÊñôÈÖí„ÄÅÁõê„ÄÅÊ∑ÄÁ≤âËÖåÂà∂ 10 ÂàÜÈíüÂ∑¶Âè≥Ôºõ\n",
       "\n",
       "2. Âπ≤Ëæ£Ê§íÂâ™ÊàêÊÆµÔºåËë±ÂßúËíúÂàáÊú´Â§áÁî®Ôºõ\n",
       "\n",
       "3. ÁÉ≠ÈîÖÂáâÊ≤πÔºåÂ∞ÜËÖåÂà∂Â•ΩÁöÑÈ∏°‰∏ÅÊîæÂÖ•ÈîÖ‰∏≠ÁÖ∏ÁÇíËá≥ÂèòËâ≤Ôºõ\n",
       "\n",
       "4. ÁõõÂá∫Â§áÁî®Ôºõ\n",
       "\n",
       "5. ÈîÖ‰∏≠ÁïôÂ∫ïÊ≤πÔºåÊîæÂÖ•Ëë±ÂßúËíúÊú´ÁÖ∏ÁÇíÂá∫È¶ôÂë≥ÔºåÂä†ÂÖ•Âπ≤Ëæ£Ê§íÁÖ∏ÁÇíËá≥È¶ôÔºõ\n",
       "\n",
       "6. Â∞ÜÁÇíÂ•ΩÁöÑÂπ≤Ëæ£Ê§íÂíåÈ∏°‰∏ÅÊîæÂÖ•ÈîÖ‰∏≠ÔºåÂä†ÂÖ•ÈÄÇÈáèÁöÑÁîüÊäΩ„ÄÅÈÜã„ÄÅÁ≥ñ„ÄÅÁõê„ÄÅÊñôÈÖíÔºåÁøªÁÇíÂùáÂåÄÔºõ\n",
       "\n",
       "7. Âä†ÂÖ•ÈÄÇÈáèÁöÑÊ∞¥ÔºåÁÑñÁÖÆ 3 ÂàÜÈíüÂ∑¶Âè≥ÔºåÊî∂Ê±ÅÂç≥ÂèØ„ÄÇ\n",
       "\n",
       "Ê∏©È¶®ÊèêÁ§∫Ôºö\n",
       "\n",
       "1. Âπ≤Ëæ£Ê§íÂèØ‰ª•Ê†πÊçÆ‰∏™‰∫∫Âè£Âë≥ÈÄÇÈáèË∞ÉÊï¥Ôºõ\n",
       "\n",
       "2. È∏°‰∏ÅÁÖ∏ÁÇíËá≥ÂèòËâ≤Âç≥ÂèØÔºåËøáÂ∫¶ÁÖ∏ÁÇí‰ºöÂΩ±ÂìçÂè£ÊÑüÔºõ\n",
       "\n",
       "3. ÁÑñÁÖÆÁöÑÊó∂Èó¥ÂèØ‰ª•Ê†πÊçÆËá™Â∑±ÁöÑÂè£ÊÑüË∞ÉÊï¥ÔºåÂ¶ÇÊûúÂñúÊ¨¢Âè£ÊÑüËÑÜÁàΩÔºåÂèØ‰ª•ÈÄÇÂΩìÂª∂ÈïøÁÑñÁÖÆÊó∂Èó¥„ÄÇ"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Â¶Ç‰ΩïÂà∂‰ΩúÂÆ´‰øùÈ∏°‰∏Å\"\n",
    "for response, history in model.stream_chat(\n",
    "        tokenizer, prompt, history=[]):\n",
    "    clear_output(wait=True)\n",
    "    display(Markdown(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
